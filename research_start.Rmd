---
title: "Research"
author: "Sukmin Lee & Sunho Tae"
date: "`r Sys.Date()`"
documentclass: article
geometry: margin=1in
fontsize: 11pt
output:
  pdf_document:
    toc: false
    df_print: kable
    fig_caption: false
    number_sections: false
    dev: pdf
    highlight: tango
  html_document:
    theme: default
    self_contained: true
    toc: false
    df_print: kable
    fig_caption: false
    number_sections: false
    smart: true
    dev: svg
---

```{r setup, include = FALSE}
# DO NOT ALTER THIS CHUNK
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  fig.width = 5,
  fig.asp = 0.618,
  out.width = "70%",
  dpi = 120,
  fig.align = "center",
  cache = FALSE
)
is_pdf <- try (("pdf_document" %in% rmarkdown::all_output_formats(knitr::current_input())), silent=TRUE)
is_pdf <- (is_pdf == TRUE)
# Load required packages
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(broom))
suppressPackageStartupMessages(library(modelr))
suppressPackageStartupMessages(library(plotly))
# Load dataset
car_prices <- read_rds("car_price_data.rds")
```


## Exercise 1

Two continuous variables of the dataset are "price" and "mileage."


```{r}
car_prices %>%
  pivot_longer(cols = Mileage | Liter,
    names_to = "category",
    values_to = "values") %>%
  
  ggplot() +
  geom_point(mapping = aes(x = values, y = Price)) +
  facet_wrap(~category, scales = "free_x") +
  labs(title = "The point plot about Mileage",
       x = "value",
       y = "price")
```



## Exercise 2

```{r}
continuous_model <- lm(Price ~ Mileage + Liter, data = car_prices)
```

```{r}
continuous_model %>%
  tidy()
```
```{r}
continuous_model %>%
glance () %>% 
  select(r.squared)
```

The r.squared value of the dataset is 0.3291279, which is considered as an okay effect size. 
 
## Exercise 3

```{r}
# predict model plane over values
lit <- unique(car_prices$Liter)
mil <- unique(car_prices$Mileage)
grid <- with(car_prices, expand.grid(lit, mil))
d <- setNames(data.frame(grid), c("Liter", "Mileage"))
vals <- predict(continuous_model, newdata = d)

# form surface matrix and give to plotly
m <- matrix(vals, nrow = length(unique(d$Liter)), ncol = length(unique(d$Mileage)))
p <- plot_ly() %>%
  add_markers(
    x = ~car_prices$Mileage, 
    y = ~car_prices$Liter, 
    z = ~car_prices$Price, 
    marker = list(size = 1)
    ) %>%
  add_trace(
    x = ~mil, y = ~lit, z = ~m, type="surface", 
    colorscale=list(c(0,1), c("yellow","yellow")),
    showscale = FALSE
    ) %>%
  layout(
    scene = list(
      xaxis = list(title = "mileage"),
      yaxis = list(title = "liters"),
      zaxis = list(title = "price")
    )
  )
if (!is_pdf) {p}
```

The model fits the data well. However, I prefer to use the 2D graph because 2D model is easier than 3D model to examine the three assumptions of the linear model.


## Exercise 4

```{r}
continuous_df <- car_prices %>%
  add_predictions(continuous_model) %>%
  add_residuals(continuous_model)
```


## Exercise 5

```{r}
continuous_df %>%
  ggplot() +
  geom_point(mapping = aes(x = pred, y = Price)) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = "Observed vs Predicted plots",
       x = "Predicted",
       y = "Price")
```
The graph follows linearity as there is an obvious linear trend.

## Exercise 6

```{r}
continuous_df %>%
  ggplot() +
  geom_point(mapping = aes(x = pred, y = resid)) +
  geom_hline(yintercept =  0) +
  labs(title = "Residual vs Predicted Plot",
       x = "Predicted",
       y = "Residual")
```
The above plot violates the linear model's assumption of constant variability in the residuals because the residuals located above the line of best fit and below the line of best fit are clearly inconsistent. 


## Exercise 7

```{r}
continuous_df %>%
  ggplot() +
  geom_qq(aes(sample = resid)) +
  geom_qq_line(aes(sample = resid)) +
  labs(title = "Q-Q Plot",
       x = "theoretical",
       y = "sample")
```
The above QQ plot violates the assumption of nearly normal residuals as the residuals aren't normally distributed.

## Exercise 8

```{r}
car_prices %>%
  ggplot() +
  geom_boxplot(aes(x = Make, y = Price)) +
  labs(x = "Make of car", title = "Effect of make of car on price")
```
1. Saturn has the lowest median price.
2. Cadillac has the greatest interquartile range of prices.
3. Cadillac, Chevrolet, Pontiac have outliers.

## Exercise 9

```{r fig.width=16}
car_prices %>%
  pivot_longer(cols = Make:Cylinder | Doors:Leather,
               names_to = "column", values_to = "value",
               values_transform = list(value = 'factor')) %>%
  ggplot() +
  geom_boxplot(aes(x = reorder(value, Price, FUN = median), y = Price)) +
  facet_wrap(~column, scales = "free_x") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  labs(title = "Categorical Variables Boxplot",
       x = "categorial variables",
       y = "price")
```


## Exercise 10

```{r}
cars_factor_df <- car_prices %>%
  mutate(Cylinder = as.factor(Cylinder))
```

1.
```{r}
mixed_model <- lm(Price ~ Liter + Mileage + Cylinder + Make + Type,
                  data = cars_factor_df)
```

2. 
```{r}
mixed_model %>%
  tidy()
```
3. 

```{r}
mixed_model %>%
  glance() %>%
  select(r.squared)
```

## Exercise 11

1.
```{r}
mixed_df <- cars_factor_df %>%
  add_predictions(mixed_model) %>%
  add_residuals(mixed_model)
```

2. 
```{r}
mixed_df %>%
ggplot() +
  geom_point(mapping = aes(x = pred, y = Price)) +
  geom_abline(slope = 1, intercept = 0) +
  labs(title = "observed vs predicted Plot",
       x = "predicted",
       y = "Price")
```

```{r}
mixed_df %>%
  ggplot() +
  geom_point(mapping = aes(x = pred, y = resid)) +
  geom_hline(yintercept = 0) +
  labs(title = "residual vs. predicted plot",
       x = "predicted",
       y = "residuals"
       )
```

```{r}
mixed_df %>%
  ggplot() +
  geom_qq(aes(sample = resid)) +
  geom_qq_line(aes(sample = resid)) +
  labs(title = "Q-Q plot",
       x = "theoretical",
       y = "sample"
       )
```

## Exercise 12

1. As seen above, the mixed model's r.squared value is 0.9389165 and the simple model's r.squared value is 0.3291279, which implies that the mixed model fits much better to the dataset because the closer r.squared value to 1 implies better data. In terms of three assumptions of the linear model, the mixed model meets the linearity condition, but violates constant variability condition and the nearly normal distribution condition. The simple model also meets the linearity condition, but violates constant variability and the nearly normal distribution condition. 


2. As I answered above, both mixed model and simple model meet one condition, but violate two conditions, in terms of three assumptions of the linear model. However, since the mixed model has so much higher r.squared value than the simple model, I'd definitely use the mixed model. 


